{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5d6afbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------------\n",
    "#       CNG 483 COMPUTER VISION PRACTICAL ASSIGNMENT 2\n",
    "#       OMAR R.A. AMMAR     |   2456010\n",
    "# ---------------------------------------------------------------------------------\n",
    "# Importing required libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import re\n",
    "import os\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "# MLP with manual validation set\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# getting the random seed for initializing the weights of the parameters\n",
    "np.random.seed(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46cf9f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "       return np.maximum(0, Z),Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c98bae34",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################\n",
    "#               Loading the datasets and separate them                           #\n",
    "###################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e5891f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to import all the images from the file with its labels\n",
    "\n",
    "def loadAllImages(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith(\".jpg\"):\n",
    "            image = Image.open(os.path.join(folder, filename)).convert('RGB')\n",
    "            image = image.resize((256, 256))\n",
    "            npimage = np.array(image)\n",
    "            if image is not None:\n",
    "                # Using re.compile() + re.match() + re.groups()\n",
    "                # Splitting text and number in string\n",
    "                temp = re.compile(\"([a-zA-Z]+)([0-9]+)\")\n",
    "                res = temp.match(filename).groups()\n",
    "                images.append((npimage,res[0]))\n",
    "    \n",
    "    return images\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e158e99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# a function that'll separate the dataset into 3 datasets\n",
    "def separateDataSets(dataset):\n",
    "    # first shuffle the dataset\n",
    "    np.random.shuffle(dataset)\n",
    "    \n",
    "    datasetImages = []\n",
    "    datasetLabels = []\n",
    "    \n",
    "    for image in dataset:\n",
    "        datasetImages.append(image[0])\n",
    "        datasetLabels.append(image[1])\n",
    "    \n",
    "    \n",
    "    train_pct_index = int(0.6 * len(dataset))\n",
    "    val_pct_index = int(train_pct_index+0.2*(len(dataset)))\n",
    "        \n",
    "    X_train,X_val, X_test = datasetImages[:train_pct_index], datasetImages[train_pct_index:val_pct_index], datasetImages[val_pct_index:]\n",
    "    y_train,y_val ,y_test = datasetLabels[:train_pct_index], datasetLabels[train_pct_index:val_pct_index], datasetLabels[val_pct_index:]\n",
    "    \n",
    "    X_train1 = np.array(X_train).astype('float32')\n",
    "    X_test1 = np.array(X_test).astype('float32')\n",
    "    X_val1 = np.array(X_val).astype('float32')\n",
    "\n",
    "\n",
    "    X_train1 /= 255\n",
    "    X_test1 /= 255\n",
    "    X_val1 /= 255\n",
    "\n",
    "    \n",
    "    return X_train1, y_train, X_val1, y_val ,X_test1, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e80ea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the functions below is my try at building my model froms scratch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49d251f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################\n",
    "#               Initialization of the parameters                                  #\n",
    "###################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bee4ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initializeParameters():\n",
    "    # create the parameters dictionary\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)  # number of layers in the network\n",
    "\n",
    "    # initialize the weights in all the layers\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95347356",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###################################################################################\n",
    "#               Forward and Backward Propagation                                  #\n",
    "###################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d46485a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardPropogation(X, W, b):\n",
    "    # Z= WX+b\n",
    "    Z = np.dot(W, X) + b\n",
    "\n",
    "    cache = (X, W, b)\n",
    "\n",
    "    return Z, cache\n",
    "\n",
    "\n",
    "def linearActivation(A_prev, W, b):\n",
    "    # I used relu as our activation function for forward propogation\n",
    "    Z, linear_cache = forwardPropogation(A_prev, W, b)\n",
    "    A, activation_cache = relu(Z)\n",
    "\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2  # number of layers in the neural network\n",
    "\n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        W = parameters['W' + str(l)]  # Get the parameters from the parameters dictionary\n",
    "        b = parameters['b' + str(l)]\n",
    "        A, cache = linearActivation(A_prev, W, b)\n",
    "        caches.append(cache)\n",
    "\n",
    "    W = parameters['W' + str(L)]\n",
    "    b = parameters['b' + str(L)]\n",
    "    AL, cache = linearActivation(A, W, b)\n",
    "    caches.append(cache)\n",
    "\n",
    "    return AL, caches\n",
    "\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = (1 / m) * np.dot(dZ, A_prev.T)\n",
    "    db = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "\n",
    "def linear_activation_backward(dA, cache):\n",
    "    linear_cache, activation_cache = cache\n",
    "\n",
    "    dZ = relu_backward(dA, activation_cache)\n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2  # number of layers in the neural network\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "\n",
    "    return parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "337c88ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###################################################################################\n",
    "#               Computing the cost                                               #\n",
    "###################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e91dab7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    # Compute loss from aL and y.\n",
    "\n",
    "    cost = (-1 / m) * (np.dot(Y, np.log(AL).T) + np.dot((1 - Y), np.log(1 - AL).T))  #\n",
    "\n",
    "    cost = np.squeeze(cost)  # To make sure cost's shape is what we expect\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7faf756",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###################################################################################\n",
    "#               Building the model                                                #\n",
    "###################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fad1dc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=False, plot_graph=False):\n",
    "    costs = []  # keep track of cost\n",
    "\n",
    "    # Parameters initialization.\n",
    "    parameters = initializeParameters()\n",
    "\n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "        # forward pass\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "\n",
    "        # Compute cost.\n",
    "        cost = compute_cost(AL, Y)\n",
    "\n",
    "        # Backward pass.\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "\n",
    "        # Update parameters after every pass.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "            costs.append(cost)\n",
    "\n",
    "    # plot the cost\n",
    "    if plot_graph:\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "    return parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43c7de84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###################################################################################\n",
    "#         Model's predictions vs the actual labels                               #\n",
    "###################################################################################\n",
    "\n",
    "def predict(images, labels, parameters):\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e171074",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load all the images from the folder\n",
    "# CHANGE THE NEXT LINE WITH YOUR OWN PATH\n",
    "dataset = loadAllImages(\"/Users/omarammar/PycharmProjects/ComVis2/Dataset\")   \n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d22c5173",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05156833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the size of every image in the dataset:  (256, 256, 3)\n",
      "number of examples in the dataset:  907\n"
     ]
    }
   ],
   "source": [
    "print('the size of every image in the dataset: ',dataset[0][0].shape)\n",
    "print('number of examples in the dataset: ',len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7376abf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val,X_test, y_test = separateDataSets(dataset) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0fce2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the training set: 544\n",
      "Number of images in the validation set: 181\n",
      "Number of images in the test set: 182\n",
      "size of images in the training set (256, 256, 3)\n",
      "The size of the training set:  (544, 256, 256, 3)\n",
      "The size of the test set:  (182, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "# make sure of the training and testing data shapes\n",
    "print('Number of images in the training set: '+ str(X_train.shape[0]))\n",
    "print('Number of images in the validation set: '+ str(X_val.shape[0]))\n",
    "print('Number of images in the test set: '+ str(X_test.shape[0]))\n",
    "print('size of images in the training set',X_train[0].shape)\n",
    "print('The size of the training set: ',X_train.shape)\n",
    "print('The size of the test set: ',X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "132ee382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot coded labels \n",
    "y_train_new = []\n",
    "for y in y_train:\n",
    "    if y == 'cloudy':\n",
    "        y_train_new.append([0,0,1])\n",
    "    elif y == 'sunrise':\n",
    "        y_train_new.append([0,1,0])\n",
    "    else:\n",
    "        y_train_new.append([1,0,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ec481b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61174c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot coded labels \n",
    "y_test_new = []\n",
    "for y in y_test:\n",
    "    if y == 'cloudy':\n",
    "        y_test_new.append([0,0,1])\n",
    "    elif y == 'sunrise':\n",
    "        y_test_new.append([0,1,0])\n",
    "    else:\n",
    "        y_test_new.append([1,0,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "42f95ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.array(y_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc0fb979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot coded labels \n",
    "y_val_new = []\n",
    "for y in y_val:\n",
    "    if y == 'cloudy':\n",
    "        y_val_new.append([0,0,1])\n",
    "    elif y == 'sunrise':\n",
    "        y_val_new.append([0,1,0])\n",
    "    else:\n",
    "        y_val_new.append([1,0,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "18b8e6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = np.array(y_val_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1a8332f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-17 11:38:20.892232: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 7s 111ms/step - loss: 19.0431 - accuracy: 0.7040 - val_loss: 33.5680 - val_accuracy: 0.6133\n",
      "Epoch 2/50\n",
      "55/55 [==============================] - 7s 113ms/step - loss: 8.8729 - accuracy: 0.7923 - val_loss: 11.5191 - val_accuracy: 0.6409\n",
      "Epoch 3/50\n",
      "55/55 [==============================] - 6s 113ms/step - loss: 12.0969 - accuracy: 0.7463 - val_loss: 4.3414 - val_accuracy: 0.8177\n",
      "Epoch 4/50\n",
      "55/55 [==============================] - 6s 111ms/step - loss: 1.8617 - accuracy: 0.8419 - val_loss: 1.8508 - val_accuracy: 0.8287\n",
      "Epoch 5/50\n",
      "55/55 [==============================] - 6s 118ms/step - loss: 1.8381 - accuracy: 0.7978 - val_loss: 2.6416 - val_accuracy: 0.8287\n",
      "Epoch 6/50\n",
      "55/55 [==============================] - 6s 113ms/step - loss: 1.4331 - accuracy: 0.8548 - val_loss: 2.5819 - val_accuracy: 0.8011\n",
      "Epoch 7/50\n",
      "55/55 [==============================] - 6s 109ms/step - loss: 0.9565 - accuracy: 0.8879 - val_loss: 2.1558 - val_accuracy: 0.7514\n",
      "Epoch 8/50\n",
      "55/55 [==============================] - 6s 109ms/step - loss: 0.5556 - accuracy: 0.8805 - val_loss: 1.5390 - val_accuracy: 0.8508\n",
      "Epoch 9/50\n",
      "55/55 [==============================] - 6s 109ms/step - loss: 0.4305 - accuracy: 0.9136 - val_loss: 1.6475 - val_accuracy: 0.8508\n",
      "Epoch 10/50\n",
      "55/55 [==============================] - 6s 105ms/step - loss: 0.3324 - accuracy: 0.9026 - val_loss: 1.1660 - val_accuracy: 0.8508\n",
      "Epoch 11/50\n",
      "55/55 [==============================] - 6s 111ms/step - loss: 0.3126 - accuracy: 0.9301 - val_loss: 1.2927 - val_accuracy: 0.8729\n",
      "Epoch 12/50\n",
      "55/55 [==============================] - 6s 113ms/step - loss: 0.9323 - accuracy: 0.8548 - val_loss: 1.5058 - val_accuracy: 0.8066\n",
      "Epoch 13/50\n",
      "55/55 [==============================] - 7s 125ms/step - loss: 1.4650 - accuracy: 0.8327 - val_loss: 1.9463 - val_accuracy: 0.8343\n",
      "Epoch 14/50\n",
      "55/55 [==============================] - 7s 129ms/step - loss: 1.1220 - accuracy: 0.8438 - val_loss: 1.6383 - val_accuracy: 0.7790\n",
      "Epoch 15/50\n",
      "55/55 [==============================] - 7s 135ms/step - loss: 0.7292 - accuracy: 0.8768 - val_loss: 1.7230 - val_accuracy: 0.8122\n",
      "Epoch 16/50\n",
      "55/55 [==============================] - 8s 144ms/step - loss: 0.3304 - accuracy: 0.9118 - val_loss: 1.8577 - val_accuracy: 0.8066\n",
      "Epoch 17/50\n",
      "55/55 [==============================] - 7s 119ms/step - loss: 0.3598 - accuracy: 0.8971 - val_loss: 1.4543 - val_accuracy: 0.8177\n",
      "Epoch 18/50\n",
      "55/55 [==============================] - 6s 114ms/step - loss: 0.2521 - accuracy: 0.9283 - val_loss: 1.2376 - val_accuracy: 0.8232\n",
      "Epoch 19/50\n",
      "55/55 [==============================] - 6s 113ms/step - loss: 0.4112 - accuracy: 0.9007 - val_loss: 1.2727 - val_accuracy: 0.8674\n",
      "Epoch 20/50\n",
      "55/55 [==============================] - 6s 116ms/step - loss: 0.5464 - accuracy: 0.8548 - val_loss: 1.8358 - val_accuracy: 0.7072\n",
      "Epoch 21/50\n",
      "55/55 [==============================] - 6s 114ms/step - loss: 0.4465 - accuracy: 0.9007 - val_loss: 2.5113 - val_accuracy: 0.7901\n",
      "Epoch 22/50\n",
      "55/55 [==============================] - 6s 113ms/step - loss: 0.4400 - accuracy: 0.8934 - val_loss: 1.1913 - val_accuracy: 0.8287\n",
      "Epoch 23/50\n",
      "55/55 [==============================] - 6s 111ms/step - loss: 0.2431 - accuracy: 0.9154 - val_loss: 1.1602 - val_accuracy: 0.8343\n",
      "Epoch 24/50\n",
      "55/55 [==============================] - 6s 112ms/step - loss: 0.2007 - accuracy: 0.9265 - val_loss: 0.8438 - val_accuracy: 0.8508\n",
      "Epoch 25/50\n",
      "55/55 [==============================] - 6s 112ms/step - loss: 0.1242 - accuracy: 0.9614 - val_loss: 1.4950 - val_accuracy: 0.7845\n",
      "Epoch 26/50\n",
      "55/55 [==============================] - 6s 110ms/step - loss: 0.2150 - accuracy: 0.9265 - val_loss: 1.1625 - val_accuracy: 0.8232\n",
      "Epoch 27/50\n",
      "55/55 [==============================] - 7s 117ms/step - loss: 0.1603 - accuracy: 0.9357 - val_loss: 0.9355 - val_accuracy: 0.8729\n",
      "Epoch 28/50\n",
      "55/55 [==============================] - 6s 114ms/step - loss: 0.2135 - accuracy: 0.9320 - val_loss: 1.8942 - val_accuracy: 0.7624\n",
      "Epoch 29/50\n",
      "55/55 [==============================] - 7s 117ms/step - loss: 0.3978 - accuracy: 0.8860 - val_loss: 1.3240 - val_accuracy: 0.8508\n",
      "Epoch 30/50\n",
      "55/55 [==============================] - 6s 108ms/step - loss: 0.1471 - accuracy: 0.9596 - val_loss: 1.2477 - val_accuracy: 0.8729\n",
      "Epoch 31/50\n",
      "55/55 [==============================] - 6s 110ms/step - loss: 0.1240 - accuracy: 0.9467 - val_loss: 1.3144 - val_accuracy: 0.8729\n",
      "Epoch 32/50\n",
      "55/55 [==============================] - 6s 111ms/step - loss: 0.1615 - accuracy: 0.9430 - val_loss: 1.7279 - val_accuracy: 0.8232\n",
      "Epoch 33/50\n",
      "55/55 [==============================] - 6s 117ms/step - loss: 0.1483 - accuracy: 0.9467 - val_loss: 1.6349 - val_accuracy: 0.8287\n",
      "Epoch 34/50\n",
      "55/55 [==============================] - 6s 111ms/step - loss: 0.1285 - accuracy: 0.9522 - val_loss: 1.3684 - val_accuracy: 0.8895\n",
      "Epoch 35/50\n",
      "55/55 [==============================] - 6s 115ms/step - loss: 0.1751 - accuracy: 0.9522 - val_loss: 0.9493 - val_accuracy: 0.8287\n",
      "Epoch 36/50\n",
      "55/55 [==============================] - 6s 114ms/step - loss: 0.1191 - accuracy: 0.9559 - val_loss: 1.0493 - val_accuracy: 0.8453\n",
      "Epoch 37/50\n",
      "55/55 [==============================] - 6s 108ms/step - loss: 0.1516 - accuracy: 0.9522 - val_loss: 1.0590 - val_accuracy: 0.8343\n",
      "Epoch 38/50\n",
      "55/55 [==============================] - 6s 106ms/step - loss: 0.0858 - accuracy: 0.9706 - val_loss: 0.9790 - val_accuracy: 0.8508\n",
      "Epoch 39/50\n",
      "55/55 [==============================] - 6s 108ms/step - loss: 0.0700 - accuracy: 0.9688 - val_loss: 1.0791 - val_accuracy: 0.8453\n",
      "Epoch 40/50\n",
      "55/55 [==============================] - 6s 112ms/step - loss: 0.1584 - accuracy: 0.9375 - val_loss: 1.5711 - val_accuracy: 0.8564\n",
      "Epoch 41/50\n",
      "55/55 [==============================] - 7s 117ms/step - loss: 0.1078 - accuracy: 0.9577 - val_loss: 1.2279 - val_accuracy: 0.8343\n",
      "Epoch 42/50\n",
      "55/55 [==============================] - 6s 111ms/step - loss: 0.0668 - accuracy: 0.9779 - val_loss: 1.4117 - val_accuracy: 0.8619\n",
      "Epoch 43/50\n",
      "55/55 [==============================] - 6s 110ms/step - loss: 0.1235 - accuracy: 0.9596 - val_loss: 1.3979 - val_accuracy: 0.8287\n",
      "Epoch 44/50\n",
      "55/55 [==============================] - 6s 111ms/step - loss: 0.2508 - accuracy: 0.9301 - val_loss: 1.2937 - val_accuracy: 0.8729\n",
      "Epoch 45/50\n",
      "55/55 [==============================] - 7s 119ms/step - loss: 0.2987 - accuracy: 0.9154 - val_loss: 1.4080 - val_accuracy: 0.8122\n",
      "Epoch 46/50\n",
      "55/55 [==============================] - 6s 110ms/step - loss: 0.1753 - accuracy: 0.9283 - val_loss: 1.3680 - val_accuracy: 0.8398\n",
      "Epoch 47/50\n",
      "55/55 [==============================] - 6s 109ms/step - loss: 0.2118 - accuracy: 0.9099 - val_loss: 0.7978 - val_accuracy: 0.8066\n",
      "Epoch 48/50\n",
      "55/55 [==============================] - 6s 111ms/step - loss: 0.4365 - accuracy: 0.8787 - val_loss: 2.4373 - val_accuracy: 0.6961\n",
      "Epoch 49/50\n",
      "55/55 [==============================] - 6s 108ms/step - loss: 0.4050 - accuracy: 0.8511 - val_loss: 1.4303 - val_accuracy: 0.6243\n",
      "Epoch 50/50\n",
      "55/55 [==============================] - 6s 108ms/step - loss: 0.2126 - accuracy: 0.9320 - val_loss: 0.8349 - val_accuracy: 0.8343\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12fe94fd0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing with the first setting \n",
    "# 2 layer model \n",
    "# input layer is a flattening layer\n",
    "# layer 1 has 256 nodes and uses relu as activation\n",
    "# layer 2 has 128 nodes and uses relu as activation\n",
    "# output layer uses softmax and has 3 nodes\n",
    "\n",
    "# create model\n",
    "model1 = Sequential()\n",
    "model1.add(keras.layers.Flatten())\n",
    "model1.add(Dense(256, activation='relu'))\n",
    "model1.add(Dense(128, activation='relu'))\n",
    "model1.add(Dense(3, activation='softmax'))\n",
    "# Compile model\n",
    "model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "model1.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=50, batch_size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5b2ccdc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    The previous model got a validation accuracy of 0.8885 , and training accuracy of 0.9210 \\n    On the other hand the loss was fairly low, for training loss it was 0.2136 and validation loss of 0.7528\\n\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    The previous model got a validation accuracy of 0.8343 , and training accuracy of 0.9320 \n",
    "    On the other hand the loss was fairly low, for training loss it was 0.2126 and validation loss of 0.8349\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f19d3af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "55/55 [==============================] - 7s 110ms/step - loss: 13.1207 - accuracy: 0.6360 - val_loss: 15.8457 - val_accuracy: 0.5304\n",
      "Epoch 2/50\n",
      "55/55 [==============================] - 6s 112ms/step - loss: 4.3093 - accuracy: 0.7831 - val_loss: 2.7252 - val_accuracy: 0.7956\n",
      "Epoch 3/50\n",
      "55/55 [==============================] - 6s 117ms/step - loss: 2.6495 - accuracy: 0.8143 - val_loss: 4.1522 - val_accuracy: 0.7017\n",
      "Epoch 4/50\n",
      "55/55 [==============================] - 6s 117ms/step - loss: 2.7814 - accuracy: 0.7923 - val_loss: 5.7406 - val_accuracy: 0.7293\n",
      "Epoch 5/50\n",
      "55/55 [==============================] - 7s 121ms/step - loss: 1.4916 - accuracy: 0.8511 - val_loss: 1.6266 - val_accuracy: 0.7845\n",
      "Epoch 6/50\n",
      "55/55 [==============================] - 6s 114ms/step - loss: 1.8274 - accuracy: 0.8272 - val_loss: 6.8332 - val_accuracy: 0.7182\n",
      "Epoch 7/50\n",
      "55/55 [==============================] - 6s 112ms/step - loss: 1.9654 - accuracy: 0.8033 - val_loss: 1.5402 - val_accuracy: 0.8508\n",
      "Epoch 8/50\n",
      "55/55 [==============================] - 6s 108ms/step - loss: 1.5089 - accuracy: 0.7904 - val_loss: 2.9312 - val_accuracy: 0.7182\n",
      "Epoch 9/50\n",
      "55/55 [==============================] - 6s 106ms/step - loss: 0.6604 - accuracy: 0.8824 - val_loss: 1.3203 - val_accuracy: 0.8011\n",
      "Epoch 10/50\n",
      "55/55 [==============================] - 6s 106ms/step - loss: 0.3527 - accuracy: 0.8732 - val_loss: 1.7023 - val_accuracy: 0.7238\n",
      "Epoch 11/50\n",
      "55/55 [==============================] - 6s 108ms/step - loss: 0.3853 - accuracy: 0.8787 - val_loss: 0.8670 - val_accuracy: 0.8398\n",
      "Epoch 12/50\n",
      "55/55 [==============================] - 6s 110ms/step - loss: 0.3473 - accuracy: 0.9044 - val_loss: 1.0838 - val_accuracy: 0.8232\n",
      "Epoch 13/50\n",
      "55/55 [==============================] - 10s 180ms/step - loss: 0.2666 - accuracy: 0.9210 - val_loss: 0.8791 - val_accuracy: 0.8343\n",
      "Epoch 14/50\n",
      "55/55 [==============================] - 233s 4s/step - loss: 0.2446 - accuracy: 0.9062 - val_loss: 0.8359 - val_accuracy: 0.8287\n",
      "Epoch 15/50\n",
      "55/55 [==============================] - 9s 158ms/step - loss: 0.4890 - accuracy: 0.8585 - val_loss: 2.0553 - val_accuracy: 0.6519\n",
      "Epoch 16/50\n",
      "55/55 [==============================] - 9s 163ms/step - loss: 0.4683 - accuracy: 0.8529 - val_loss: 0.9487 - val_accuracy: 0.8508\n",
      "Epoch 17/50\n",
      "55/55 [==============================] - 7s 134ms/step - loss: 0.1987 - accuracy: 0.9283 - val_loss: 1.0913 - val_accuracy: 0.8287\n",
      "Epoch 18/50\n",
      "55/55 [==============================] - 7s 120ms/step - loss: 0.1900 - accuracy: 0.9320 - val_loss: 1.0314 - val_accuracy: 0.8177\n",
      "Epoch 19/50\n",
      "55/55 [==============================] - 7s 125ms/step - loss: 0.1322 - accuracy: 0.9485 - val_loss: 0.9199 - val_accuracy: 0.8674\n",
      "Epoch 20/50\n",
      "55/55 [==============================] - 7s 126ms/step - loss: 0.2992 - accuracy: 0.8879 - val_loss: 0.9374 - val_accuracy: 0.8343\n",
      "Epoch 21/50\n",
      "55/55 [==============================] - 7s 123ms/step - loss: 0.2121 - accuracy: 0.9301 - val_loss: 0.7895 - val_accuracy: 0.8785\n",
      "Epoch 22/50\n",
      "55/55 [==============================] - 8s 150ms/step - loss: 0.1433 - accuracy: 0.9522 - val_loss: 1.0044 - val_accuracy: 0.8398\n",
      "Epoch 23/50\n",
      "55/55 [==============================] - 11s 195ms/step - loss: 0.1645 - accuracy: 0.9338 - val_loss: 0.8530 - val_accuracy: 0.8564\n",
      "Epoch 24/50\n",
      "55/55 [==============================] - 10s 183ms/step - loss: 0.1653 - accuracy: 0.9357 - val_loss: 0.9178 - val_accuracy: 0.8619\n",
      "Epoch 25/50\n",
      "55/55 [==============================] - 9s 169ms/step - loss: 0.1015 - accuracy: 0.9706 - val_loss: 0.9968 - val_accuracy: 0.8895\n",
      "Epoch 26/50\n",
      "55/55 [==============================] - 14s 236ms/step - loss: 0.1446 - accuracy: 0.9485 - val_loss: 0.9212 - val_accuracy: 0.8785\n",
      "Epoch 27/50\n",
      "55/55 [==============================] - 9s 158ms/step - loss: 0.1041 - accuracy: 0.9614 - val_loss: 0.9912 - val_accuracy: 0.8453\n",
      "Epoch 28/50\n",
      "55/55 [==============================] - 9s 153ms/step - loss: 0.1160 - accuracy: 0.9540 - val_loss: 1.1145 - val_accuracy: 0.8840\n",
      "Epoch 29/50\n",
      "55/55 [==============================] - 8s 131ms/step - loss: 0.1181 - accuracy: 0.9559 - val_loss: 1.0304 - val_accuracy: 0.8674\n",
      "Epoch 30/50\n",
      "55/55 [==============================] - 7s 126ms/step - loss: 0.1597 - accuracy: 0.9320 - val_loss: 0.9802 - val_accuracy: 0.8729\n",
      "Epoch 31/50\n",
      "55/55 [==============================] - 7s 119ms/step - loss: 0.2257 - accuracy: 0.9265 - val_loss: 0.7930 - val_accuracy: 0.8674\n",
      "Epoch 32/50\n",
      "55/55 [==============================] - 8s 150ms/step - loss: 0.2827 - accuracy: 0.9191 - val_loss: 1.0856 - val_accuracy: 0.8343\n",
      "Epoch 33/50\n",
      "55/55 [==============================] - 9s 160ms/step - loss: 0.2682 - accuracy: 0.9338 - val_loss: 0.7177 - val_accuracy: 0.8619\n",
      "Epoch 34/50\n",
      "55/55 [==============================] - 7s 120ms/step - loss: 0.1617 - accuracy: 0.9375 - val_loss: 2.4905 - val_accuracy: 0.6519\n",
      "Epoch 35/50\n",
      "55/55 [==============================] - 8s 136ms/step - loss: 0.6747 - accuracy: 0.8364 - val_loss: 1.2581 - val_accuracy: 0.7569\n",
      "Epoch 36/50\n",
      "55/55 [==============================] - 9s 159ms/step - loss: 0.2728 - accuracy: 0.9044 - val_loss: 1.3380 - val_accuracy: 0.8177\n",
      "Epoch 37/50\n",
      "55/55 [==============================] - 8s 142ms/step - loss: 0.3137 - accuracy: 0.9099 - val_loss: 0.9417 - val_accuracy: 0.8343\n",
      "Epoch 38/50\n",
      "55/55 [==============================] - 9s 153ms/step - loss: 0.1948 - accuracy: 0.9320 - val_loss: 0.9618 - val_accuracy: 0.8619\n",
      "Epoch 39/50\n",
      "55/55 [==============================] - 8s 139ms/step - loss: 0.1820 - accuracy: 0.9357 - val_loss: 0.8436 - val_accuracy: 0.8674\n",
      "Epoch 40/50\n",
      "55/55 [==============================] - 8s 138ms/step - loss: 0.1213 - accuracy: 0.9485 - val_loss: 0.8923 - val_accuracy: 0.8674\n",
      "Epoch 41/50\n",
      "55/55 [==============================] - 7s 121ms/step - loss: 0.2756 - accuracy: 0.9081 - val_loss: 1.1001 - val_accuracy: 0.8619\n",
      "Epoch 42/50\n",
      "55/55 [==============================] - 7s 134ms/step - loss: 0.2026 - accuracy: 0.9283 - val_loss: 1.1841 - val_accuracy: 0.8343\n",
      "Epoch 43/50\n",
      "55/55 [==============================] - 6s 114ms/step - loss: 0.1932 - accuracy: 0.9430 - val_loss: 1.3114 - val_accuracy: 0.8453\n",
      "Epoch 44/50\n",
      "55/55 [==============================] - 7s 122ms/step - loss: 0.1638 - accuracy: 0.9412 - val_loss: 1.0764 - val_accuracy: 0.8619\n",
      "Epoch 45/50\n",
      "55/55 [==============================] - 7s 120ms/step - loss: 0.1841 - accuracy: 0.9283 - val_loss: 1.3424 - val_accuracy: 0.8177\n",
      "Epoch 46/50\n",
      "55/55 [==============================] - 7s 124ms/step - loss: 0.1164 - accuracy: 0.9522 - val_loss: 1.2924 - val_accuracy: 0.8729\n",
      "Epoch 47/50\n",
      "55/55 [==============================] - 7s 133ms/step - loss: 0.0915 - accuracy: 0.9743 - val_loss: 1.5295 - val_accuracy: 0.8619\n",
      "Epoch 48/50\n",
      "55/55 [==============================] - 9s 165ms/step - loss: 0.1575 - accuracy: 0.9338 - val_loss: 2.0152 - val_accuracy: 0.8122\n",
      "Epoch 49/50\n",
      "55/55 [==============================] - 10s 183ms/step - loss: 0.1759 - accuracy: 0.9393 - val_loss: 1.4203 - val_accuracy: 0.8287\n",
      "Epoch 50/50\n",
      "55/55 [==============================] - 8s 144ms/step - loss: 0.0772 - accuracy: 0.9669 - val_loss: 1.1383 - val_accuracy: 0.8729\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12ece3d90>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing with the first setting \n",
    "# 3 layer model \n",
    "# input layer is a flattening layer\n",
    "# layer 1 has 256 nodes and uses relu as activation\n",
    "# layer 2 has 128 nodes and uses relu as activation\n",
    "# layer 3 has 64 nodes and uses relu as activation\n",
    "# output layer uses softmax and has 3 nodes\n",
    "\n",
    "# create model\n",
    "model2 = Sequential()\n",
    "model2.add(keras.layers.Flatten())\n",
    "model2.add(Dense(256, activation='relu'))\n",
    "model2.add(Dense(128, activation='relu'))\n",
    "model2.add(Dense(64, activation='relu'))\n",
    "model2.add(Dense(3, activation='softmax'))\n",
    "# Compile model\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "model2.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=50, batch_size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "43874a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    The previous model got a validation accuracy of 0.8177 , and training accuracy of 0.9320 \\n    On the other hand the loss was fairly high, for training loss it was 0.2562 and validation loss of 0.9942\\n\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    The previous model got a validation accuracy of 0.8729 , and training accuracy of 0.9669 \n",
    "    On the other hand the loss was fairly high, for training loss it was 0.0772 and validation loss of 0.8729\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ca06ef78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing with the first setting \n",
    "# 3 layer model \n",
    "# input layer is a flattening layer\n",
    "# layer 1 has 1024 nodes and uses relu as activation\n",
    "# layer 2 has 512 nodes and uses relu as activation\n",
    "# layer 3 has 256 nodes and uses relu as activation\n",
    "# output layer uses softmax and has 3 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd98c65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "55/55 [==============================] - 153s 3s/step - loss: 32.4843 - accuracy: 0.6562 - val_loss: 4.3592 - val_accuracy: 0.7624\n",
      "Epoch 2/50\n",
      "55/55 [==============================] - 238s 4s/step - loss: 4.2036 - accuracy: 0.7555 - val_loss: 10.2568 - val_accuracy: 0.6796\n",
      "Epoch 3/50\n",
      "55/55 [==============================] - 206s 4s/step - loss: 2.1639 - accuracy: 0.8088 - val_loss: 2.5003 - val_accuracy: 0.7072\n",
      "Epoch 4/50\n",
      "55/55 [==============================] - 154s 3s/step - loss: 1.4345 - accuracy: 0.8051 - val_loss: 2.8899 - val_accuracy: 0.6851\n",
      "Epoch 5/50\n",
      "55/55 [==============================] - 141s 3s/step - loss: 1.4721 - accuracy: 0.8033 - val_loss: 1.7736 - val_accuracy: 0.7127\n",
      "Epoch 6/50\n",
      "55/55 [==============================] - 134s 2s/step - loss: 0.6616 - accuracy: 0.8529 - val_loss: 1.0097 - val_accuracy: 0.8619\n",
      "Epoch 7/50\n",
      "55/55 [==============================] - 125s 2s/step - loss: 0.5714 - accuracy: 0.8658 - val_loss: 1.3543 - val_accuracy: 0.8122\n",
      "Epoch 8/50\n",
      "55/55 [==============================] - 121s 2s/step - loss: 0.5544 - accuracy: 0.8566 - val_loss: 1.0153 - val_accuracy: 0.8343\n",
      "Epoch 9/50\n",
      "55/55 [==============================] - 138s 3s/step - loss: 0.3524 - accuracy: 0.9099 - val_loss: 1.0983 - val_accuracy: 0.8232\n",
      "Epoch 10/50\n",
      "55/55 [==============================] - 111s 2s/step - loss: 0.5083 - accuracy: 0.8603 - val_loss: 2.6920 - val_accuracy: 0.7017\n",
      "Epoch 11/50\n",
      "55/55 [==============================] - 126s 2s/step - loss: 0.7566 - accuracy: 0.8621 - val_loss: 1.9809 - val_accuracy: 0.7072\n",
      "Epoch 12/50\n",
      "55/55 [==============================] - 115s 2s/step - loss: 0.4512 - accuracy: 0.8750 - val_loss: 1.0326 - val_accuracy: 0.8343\n",
      "Epoch 13/50\n",
      "55/55 [==============================] - 310s 6s/step - loss: 0.3601 - accuracy: 0.8989 - val_loss: 1.8024 - val_accuracy: 0.6961\n",
      "Epoch 14/50\n",
      "55/55 [==============================] - 111s 2s/step - loss: 0.3992 - accuracy: 0.8732 - val_loss: 2.0595 - val_accuracy: 0.6796\n",
      "Epoch 15/50\n",
      "55/55 [==============================] - 107s 2s/step - loss: 0.3765 - accuracy: 0.8787 - val_loss: 1.0723 - val_accuracy: 0.8122\n",
      "Epoch 16/50\n",
      "55/55 [==============================] - 327s 6s/step - loss: 0.3536 - accuracy: 0.8750 - val_loss: 0.7807 - val_accuracy: 0.8343\n",
      "Epoch 17/50\n",
      "55/55 [==============================] - 202s 4s/step - loss: 0.2262 - accuracy: 0.9283 - val_loss: 1.2608 - val_accuracy: 0.8343\n",
      "Epoch 18/50\n",
      "55/55 [==============================] - 210s 4s/step - loss: 0.1875 - accuracy: 0.9338 - val_loss: 1.5178 - val_accuracy: 0.6906\n",
      "Epoch 19/50\n",
      "55/55 [==============================] - 167s 3s/step - loss: 0.3660 - accuracy: 0.8732 - val_loss: 0.9915 - val_accuracy: 0.8232\n",
      "Epoch 20/50\n",
      "55/55 [==============================] - 128s 2s/step - loss: 0.3029 - accuracy: 0.9007 - val_loss: 1.0145 - val_accuracy: 0.8674\n",
      "Epoch 21/50\n",
      "55/55 [==============================] - 127s 2s/step - loss: 0.3679 - accuracy: 0.8842 - val_loss: 0.9950 - val_accuracy: 0.8674\n",
      "Epoch 22/50\n",
      "55/55 [==============================] - 134s 2s/step - loss: 0.3497 - accuracy: 0.8676 - val_loss: 1.3011 - val_accuracy: 0.7790\n",
      "Epoch 23/50\n",
      "55/55 [==============================] - 131s 2s/step - loss: 0.2568 - accuracy: 0.9173 - val_loss: 1.2639 - val_accuracy: 0.8011\n",
      "Epoch 24/50\n",
      "55/55 [==============================] - 134s 2s/step - loss: 0.3576 - accuracy: 0.8971 - val_loss: 1.1698 - val_accuracy: 0.8177\n",
      "Epoch 25/50\n",
      "55/55 [==============================] - 286s 5s/step - loss: 0.2194 - accuracy: 0.9191 - val_loss: 0.9510 - val_accuracy: 0.8619\n",
      "Epoch 26/50\n",
      "55/55 [==============================] - 133s 2s/step - loss: 0.2129 - accuracy: 0.9301 - val_loss: 0.8401 - val_accuracy: 0.8564\n",
      "Epoch 27/50\n",
      "55/55 [==============================] - 134s 2s/step - loss: 0.1628 - accuracy: 0.9430 - val_loss: 1.2460 - val_accuracy: 0.8011\n",
      "Epoch 28/50\n",
      "55/55 [==============================] - 143s 3s/step - loss: 0.1176 - accuracy: 0.9669 - val_loss: 0.9989 - val_accuracy: 0.8619\n",
      "Epoch 29/50\n",
      "55/55 [==============================] - 113s 2s/step - loss: 0.1113 - accuracy: 0.9596 - val_loss: 1.2289 - val_accuracy: 0.7956\n",
      "Epoch 30/50\n",
      "55/55 [==============================] - 140s 3s/step - loss: 0.1817 - accuracy: 0.9393 - val_loss: 1.2550 - val_accuracy: 0.8564\n",
      "Epoch 31/50\n",
      "55/55 [==============================] - 122s 2s/step - loss: 0.1434 - accuracy: 0.9522 - val_loss: 1.2856 - val_accuracy: 0.8453\n",
      "Epoch 32/50\n",
      "55/55 [==============================] - 120s 2s/step - loss: 0.1203 - accuracy: 0.9559 - val_loss: 1.6817 - val_accuracy: 0.7901\n",
      "Epoch 33/50\n",
      "55/55 [==============================] - 104s 2s/step - loss: 0.1420 - accuracy: 0.9522 - val_loss: 1.2392 - val_accuracy: 0.8785\n",
      "Epoch 34/50\n",
      "55/55 [==============================] - 297s 5s/step - loss: 0.4133 - accuracy: 0.9081 - val_loss: 1.1104 - val_accuracy: 0.8398\n",
      "Epoch 35/50\n",
      "55/55 [==============================] - 1507s 28s/step - loss: 0.1885 - accuracy: 0.9412 - val_loss: 2.0171 - val_accuracy: 0.6851\n",
      "Epoch 36/50\n",
      "55/55 [==============================] - 109s 2s/step - loss: 0.2450 - accuracy: 0.9338 - val_loss: 1.0886 - val_accuracy: 0.8287\n",
      "Epoch 37/50\n",
      "55/55 [==============================] - 116s 2s/step - loss: 0.1519 - accuracy: 0.9449 - val_loss: 1.0141 - val_accuracy: 0.8619\n",
      "Epoch 38/50\n",
      "55/55 [==============================] - 111s 2s/step - loss: 0.0789 - accuracy: 0.9688 - val_loss: 1.3892 - val_accuracy: 0.8453\n",
      "Epoch 39/50\n",
      "55/55 [==============================] - 673s 12s/step - loss: 0.1560 - accuracy: 0.9357 - val_loss: 0.9656 - val_accuracy: 0.8508\n",
      "Epoch 40/50\n",
      "55/55 [==============================] - 172s 3s/step - loss: 0.0726 - accuracy: 0.9743 - val_loss: 1.1863 - val_accuracy: 0.8066\n",
      "Epoch 41/50\n",
      "55/55 [==============================] - 151s 3s/step - loss: 0.0645 - accuracy: 0.9779 - val_loss: 1.0398 - val_accuracy: 0.8564\n",
      "Epoch 42/50\n",
      "55/55 [==============================] - 145s 3s/step - loss: 0.1266 - accuracy: 0.9596 - val_loss: 1.1530 - val_accuracy: 0.8674\n",
      "Epoch 43/50\n",
      "55/55 [==============================] - 145s 3s/step - loss: 0.1069 - accuracy: 0.9577 - val_loss: 1.1396 - val_accuracy: 0.8564\n",
      "Epoch 44/50\n",
      "55/55 [==============================] - 111s 2s/step - loss: 0.0776 - accuracy: 0.9724 - val_loss: 1.1661 - val_accuracy: 0.8066\n",
      "Epoch 45/50\n",
      "55/55 [==============================] - 182s 3s/step - loss: 0.1703 - accuracy: 0.9449 - val_loss: 1.1052 - val_accuracy: 0.8619\n",
      "Epoch 46/50\n",
      "55/55 [==============================] - 170s 3s/step - loss: 0.0726 - accuracy: 0.9835 - val_loss: 1.1721 - val_accuracy: 0.8619\n",
      "Epoch 47/50\n",
      "55/55 [==============================] - 251s 5s/step - loss: 0.0366 - accuracy: 0.9908 - val_loss: 1.2256 - val_accuracy: 0.8785\n",
      "Epoch 48/50\n",
      "55/55 [==============================] - 135s 2s/step - loss: 0.1450 - accuracy: 0.9485 - val_loss: 1.4924 - val_accuracy: 0.8508\n",
      "Epoch 49/50\n",
      "55/55 [==============================] - 124s 2s/step - loss: 0.1605 - accuracy: 0.9393 - val_loss: 1.2555 - val_accuracy: 0.8619\n",
      "Epoch 50/50\n",
      "55/55 [==============================] - 101s 2s/step - loss: 0.1186 - accuracy: 0.9467 - val_loss: 1.4840 - val_accuracy: 0.8840\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12f07eb80>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# running this model would take a lot of time and power from the machine, don't run it unless u willing to wait for 2 hours\n",
    "# create model\n",
    "model3 = Sequential()\n",
    "model3.add(keras.layers.Flatten())\n",
    "model3.add(Dense(1024, activation='relu'))\n",
    "model3.add(Dense(512, activation='relu'))\n",
    "model3.add(Dense(256, activation='relu'))\n",
    "model3.add(Dense(3, activation='softmax'))\n",
    "# Compile model\n",
    "model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "model3.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=50, batch_size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a4136b1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nspike in loss at the beginning of the training\\ntraining took a very long time. because of how complex the nn got\\n The previous model got a validation accuracy of 0.8674 , and training accuracy of 0.9007 \\n    On the other hand the loss was fairly high, for training loss it was 0.2638 and validation loss of 1.3051\\n    although we added more layers and more nodes it still offered worse results than our first model with only\\n    2 layers , I belive this was because of the model overfitting.\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "spike in loss at the beginning of the training\n",
    "training took a very long time. because of how complex the nn got\n",
    " The previous model got a validation accuracy of 0.8840 , and training accuracy of 0.9467 \n",
    "    On the other hand the loss was fairly high, for training loss it was 0.1186 and validation loss of 1.4840\n",
    "    although we added more layers and more nodes it still offered worse results than our first model with only\n",
    "    2 layers , I belive this was because of the model overfitting.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7a50269d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "6/6 [==============================] - 14s 1s/step - loss: 70.0076 - accuracy: 0.4118 - val_loss: 82.2905 - val_accuracy: 0.3370\n",
      "Epoch 2/50\n",
      "6/6 [==============================] - 5s 811ms/step - loss: 33.5356 - accuracy: 0.5092 - val_loss: 17.7801 - val_accuracy: 0.5912\n",
      "Epoch 3/50\n",
      "6/6 [==============================] - 4s 703ms/step - loss: 9.9150 - accuracy: 0.6562 - val_loss: 16.8369 - val_accuracy: 0.6685\n",
      "Epoch 4/50\n",
      "6/6 [==============================] - 4s 619ms/step - loss: 8.4270 - accuracy: 0.7206 - val_loss: 3.6707 - val_accuracy: 0.7127\n",
      "Epoch 5/50\n",
      "6/6 [==============================] - 5s 793ms/step - loss: 4.5277 - accuracy: 0.7500 - val_loss: 3.0979 - val_accuracy: 0.7790\n",
      "Epoch 6/50\n",
      "6/6 [==============================] - 4s 662ms/step - loss: 2.6892 - accuracy: 0.8088 - val_loss: 4.2910 - val_accuracy: 0.7403\n",
      "Epoch 7/50\n",
      "6/6 [==============================] - 4s 613ms/step - loss: 2.5417 - accuracy: 0.7923 - val_loss: 4.7801 - val_accuracy: 0.7624\n",
      "Epoch 8/50\n",
      "6/6 [==============================] - 5s 723ms/step - loss: 2.6822 - accuracy: 0.8033 - val_loss: 4.9635 - val_accuracy: 0.7127\n",
      "Epoch 9/50\n",
      "6/6 [==============================] - 5s 774ms/step - loss: 2.6963 - accuracy: 0.8162 - val_loss: 2.7409 - val_accuracy: 0.7569\n",
      "Epoch 10/50\n",
      "6/6 [==============================] - 4s 638ms/step - loss: 2.1012 - accuracy: 0.8235 - val_loss: 2.1646 - val_accuracy: 0.7845\n",
      "Epoch 11/50\n",
      "6/6 [==============================] - 5s 751ms/step - loss: 0.9996 - accuracy: 0.8787 - val_loss: 1.4578 - val_accuracy: 0.8619\n",
      "Epoch 12/50\n",
      "6/6 [==============================] - 4s 648ms/step - loss: 0.6212 - accuracy: 0.8732 - val_loss: 1.5597 - val_accuracy: 0.8066\n",
      "Epoch 13/50\n",
      "6/6 [==============================] - 4s 693ms/step - loss: 0.6328 - accuracy: 0.8511 - val_loss: 1.6237 - val_accuracy: 0.7735\n",
      "Epoch 14/50\n",
      "6/6 [==============================] - 7s 1s/step - loss: 0.6984 - accuracy: 0.8493 - val_loss: 2.5651 - val_accuracy: 0.6464\n",
      "Epoch 15/50\n",
      "6/6 [==============================] - 5s 704ms/step - loss: 0.9520 - accuracy: 0.8419 - val_loss: 2.0097 - val_accuracy: 0.7845\n",
      "Epoch 16/50\n",
      "6/6 [==============================] - 5s 721ms/step - loss: 0.7236 - accuracy: 0.8860 - val_loss: 2.6000 - val_accuracy: 0.7348\n",
      "Epoch 17/50\n",
      "6/6 [==============================] - 5s 781ms/step - loss: 0.8303 - accuracy: 0.8585 - val_loss: 1.7511 - val_accuracy: 0.7901\n",
      "Epoch 18/50\n",
      "6/6 [==============================] - 5s 795ms/step - loss: 0.7303 - accuracy: 0.8676 - val_loss: 1.6673 - val_accuracy: 0.8011\n",
      "Epoch 19/50\n",
      "6/6 [==============================] - 6s 997ms/step - loss: 0.6689 - accuracy: 0.9026 - val_loss: 1.6868 - val_accuracy: 0.8122\n",
      "Epoch 20/50\n",
      "6/6 [==============================] - 6s 1s/step - loss: 0.4211 - accuracy: 0.9228 - val_loss: 1.3629 - val_accuracy: 0.8398\n",
      "Epoch 21/50\n",
      "6/6 [==============================] - 6s 933ms/step - loss: 0.3198 - accuracy: 0.9136 - val_loss: 1.9608 - val_accuracy: 0.7403\n",
      "Epoch 22/50\n",
      "6/6 [==============================] - 6s 943ms/step - loss: 0.3469 - accuracy: 0.8842 - val_loss: 1.2512 - val_accuracy: 0.8453\n",
      "Epoch 23/50\n",
      "6/6 [==============================] - 12s 2s/step - loss: 0.2018 - accuracy: 0.9283 - val_loss: 2.3242 - val_accuracy: 0.7182\n",
      "Epoch 24/50\n",
      "6/6 [==============================] - 9s 2s/step - loss: 0.3613 - accuracy: 0.9136 - val_loss: 1.5453 - val_accuracy: 0.8066\n",
      "Epoch 25/50\n",
      "6/6 [==============================] - 11s 2s/step - loss: 0.2565 - accuracy: 0.9320 - val_loss: 1.2585 - val_accuracy: 0.8287\n",
      "Epoch 26/50\n",
      "6/6 [==============================] - 9s 1s/step - loss: 0.3437 - accuracy: 0.9044 - val_loss: 2.1227 - val_accuracy: 0.7238\n",
      "Epoch 27/50\n",
      "6/6 [==============================] - 6s 911ms/step - loss: 0.6372 - accuracy: 0.8621 - val_loss: 1.4474 - val_accuracy: 0.8066\n",
      "Epoch 28/50\n",
      "6/6 [==============================] - 6s 1s/step - loss: 0.2834 - accuracy: 0.9210 - val_loss: 1.1390 - val_accuracy: 0.8564\n",
      "Epoch 29/50\n",
      "6/6 [==============================] - 8s 1s/step - loss: 0.3027 - accuracy: 0.9210 - val_loss: 1.3861 - val_accuracy: 0.8232\n",
      "Epoch 30/50\n",
      "6/6 [==============================] - 7s 1s/step - loss: 0.2155 - accuracy: 0.9265 - val_loss: 1.5391 - val_accuracy: 0.7735\n",
      "Epoch 31/50\n",
      "6/6 [==============================] - 6s 886ms/step - loss: 0.6437 - accuracy: 0.8511 - val_loss: 1.8089 - val_accuracy: 0.8011\n",
      "Epoch 32/50\n",
      "6/6 [==============================] - 5s 745ms/step - loss: 0.6596 - accuracy: 0.8934 - val_loss: 2.0177 - val_accuracy: 0.7735\n",
      "Epoch 33/50\n",
      "6/6 [==============================] - 5s 841ms/step - loss: 0.9214 - accuracy: 0.8401 - val_loss: 3.0836 - val_accuracy: 0.7127\n",
      "Epoch 34/50\n",
      "6/6 [==============================] - 7s 1s/step - loss: 2.7318 - accuracy: 0.7610 - val_loss: 3.2003 - val_accuracy: 0.7514\n",
      "Epoch 35/50\n",
      "6/6 [==============================] - 5s 768ms/step - loss: 1.8372 - accuracy: 0.8529 - val_loss: 3.6002 - val_accuracy: 0.7569\n",
      "Epoch 36/50\n",
      "6/6 [==============================] - 6s 980ms/step - loss: 1.6617 - accuracy: 0.8566 - val_loss: 3.8739 - val_accuracy: 0.7459\n",
      "Epoch 37/50\n",
      "6/6 [==============================] - 5s 925ms/step - loss: 1.6280 - accuracy: 0.8419 - val_loss: 4.7499 - val_accuracy: 0.5801\n",
      "Epoch 38/50\n",
      "6/6 [==============================] - 8s 1s/step - loss: 1.4943 - accuracy: 0.8088 - val_loss: 5.7621 - val_accuracy: 0.6354\n",
      "Epoch 39/50\n",
      "6/6 [==============================] - 6s 996ms/step - loss: 1.6048 - accuracy: 0.7996 - val_loss: 1.7496 - val_accuracy: 0.8564\n",
      "Epoch 40/50\n",
      "6/6 [==============================] - 5s 823ms/step - loss: 0.6423 - accuracy: 0.8971 - val_loss: 2.0140 - val_accuracy: 0.8066\n",
      "Epoch 41/50\n",
      "6/6 [==============================] - 5s 855ms/step - loss: 0.6426 - accuracy: 0.8732 - val_loss: 1.9836 - val_accuracy: 0.8343\n",
      "Epoch 42/50\n",
      "6/6 [==============================] - 7s 1s/step - loss: 0.7671 - accuracy: 0.9173 - val_loss: 2.7815 - val_accuracy: 0.7735\n",
      "Epoch 43/50\n",
      "6/6 [==============================] - 8s 1s/step - loss: 0.5925 - accuracy: 0.8897 - val_loss: 1.7172 - val_accuracy: 0.8508\n",
      "Epoch 44/50\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.4531 - accuracy: 0.9191 - val_loss: 1.9780 - val_accuracy: 0.8287\n",
      "Epoch 45/50\n",
      "6/6 [==============================] - 7s 994ms/step - loss: 0.4412 - accuracy: 0.9062 - val_loss: 1.3655 - val_accuracy: 0.8729\n",
      "Epoch 46/50\n",
      "6/6 [==============================] - 7s 1s/step - loss: 0.2719 - accuracy: 0.9228 - val_loss: 1.3660 - val_accuracy: 0.8840\n",
      "Epoch 47/50\n",
      "6/6 [==============================] - 6s 987ms/step - loss: 0.1901 - accuracy: 0.9449 - val_loss: 1.3400 - val_accuracy: 0.8895\n",
      "Epoch 48/50\n",
      "6/6 [==============================] - 6s 914ms/step - loss: 0.1577 - accuracy: 0.9393 - val_loss: 1.3456 - val_accuracy: 0.8508\n",
      "Epoch 49/50\n",
      "6/6 [==============================] - 5s 825ms/step - loss: 0.1584 - accuracy: 0.9430 - val_loss: 1.4018 - val_accuracy: 0.8895\n",
      "Epoch 50/50\n",
      "6/6 [==============================] - 3s 574ms/step - loss: 0.0966 - accuracy: 0.9688 - val_loss: 1.8132 - val_accuracy: 0.7680\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12f498760>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing with the first setting \n",
    "# 4 layer model \n",
    "# input layer is a flattening layer\n",
    "# layer 1 has 512 nodes and uses relu as activation\n",
    "# layer 2 has 256 nodes and uses relu as activation\n",
    "# layer 3 has 128 nodes and uses relu as activation\n",
    "# layer 4 has 64 nodes and uses relu as activation\n",
    "# output layer uses softmax and has 3 nodes\n",
    "\n",
    "\n",
    "# create model\n",
    "model4 = Sequential()\n",
    "model4.add(keras.layers.Flatten())\n",
    "model4.add(Dense(512, activation='relu'))\n",
    "model4.add(Dense(256, activation='relu'))\n",
    "model4.add(Dense(128, activation='relu'))\n",
    "model4.add(Dense(64, activation='relu'))\n",
    "model4.add(Dense(3, activation='softmax'))\n",
    "# Compile model\n",
    "model4.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "model4.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=50, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5a215966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    spike in the loss at the beginning of the training\\n    The previous model got a validation accuracy of 0.8619 , and training accuracy of 0.9062\\n    On the other hand the loss was fairly high, for training loss it was 0.5065 and validation loss of 1.5096\\n\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    spike in the loss at the beginning of the training\n",
    "    The previous model got a validation accuracy of 0.7680 , and training accuracy of 0.9688\n",
    "    On the other hand the loss was fairly high, for training loss it was 0.0966 and validation loss of 1.813\n",
    "    There is a big gab between the training and the validation scores which could tell us that the model is overfitting\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e19881f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnumber of layers: \\n    I tested my learning model with 3 different sizes of layers ( 2, 3, 4) layered models to compare the accuracy of \\n    each model with more layers, check the effect of number of layers in the previous question for more details.\\n    \\nnumber of epochs: \\n    I used 50 epoches for running all my models to be able to compare between all my models.\\n\\nlayer sizes: \\n    I used all my layer sizes to be multiples of 2 , because images themselves being put as an input are a multiple\\n    of 2 ( all images were resized to 256)\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###########                  QUESTIONS               #################\n",
    "\n",
    "# 1) Discussion on the effects of the number of layers.\n",
    "'''\n",
    "The number of layers in a model is referred to as its depth. \n",
    "Increasing the depth increases the capacity of the model. Training deep models, e.g. those with many hidden layers, \n",
    "can be computationally more efficient than training a single layer network with a vast number of nodes\n",
    "\n",
    "We can also see from the models I trained before that with increasing the number of layers the training time\n",
    "increases as the complexity of the model gets bigger and harder to compute.\n",
    "\n",
    "increasing the number of layers doesn't always mean having a better accuracy and loss as you can see in the comparsion \n",
    "between my 2-layer model(First trained model) and my 4-layer model (4th trained model)\n",
    "'''\n",
    "\n",
    "# 2) Rationale behind your choices of hyper-parameters like number of layers, number of epochs, layer sizes etc\n",
    "'''\n",
    "number of layers: \n",
    "    I tested my learning model with 3 different sizes of layers ( 2, 3, 4) layered models to compare the accuracy of \n",
    "    each model with more layers, check the effect of number of layers in the previous question for more details.\n",
    "    \n",
    "number of epochs: \n",
    "    I used 50 epoches for running all my models to be able to compare between all my models.\n",
    "\n",
    "layer sizes: \n",
    "    I used all my layer sizes to be multiples of 2 , because images themselves being put as an input are a multiple\n",
    "    of 2 ( all images were resized to 256)\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "87c70936",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################              Model testing with the best configuration          ##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b56ceba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we saw that the best model with the highest validation accuracy was the third model with 4 hidden layers , thus we are\n",
    "# going to run the test with this configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5ee2b532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "19/19 [==============================] - 4s 95ms/step - loss: 0.6698 - accuracy: 0.8736\n",
      "test loss, test acc: [0.6697514057159424, 0.8736263513565063]\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluate on test data\")\n",
    "results = model3.evaluate(X_test, y_test, batch_size=10)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a7388f87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    From testing model 1 , we got a fairly good accuracy of 0.8736 , I believe the accuracy could be better by \\n    varying the dataset , as we randomized the selection of the images in the dataset we could get better results\\n    \\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation of testing with the best configuration \n",
    "'''\n",
    "    From testing model 1 , we got a fairly good accuracy of 0.8736 , I believe the accuracy could be better by \n",
    "    varying the dataset , as we randomized the selection of the images in the dataset we could get better results\n",
    "    \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
